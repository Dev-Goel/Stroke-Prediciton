# -*- coding: utf-8 -*-
"""B20CS053,B20CS090,B20EE059 PRML Course Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yocAFfyv5kHHf3AzJyJ-5EE0ISA6FXnJ

# Preprocessing and visualization

### Importing necessary libraries
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
import warnings
warnings.filterwarnings("ignore")
from collections import Counter
import seaborn as sns
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
import lightgbm as lgb
from sklearn.metrics import auc, roc_auc_score, roc_curve, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from plotly.subplots import make_subplots
import plotly.graph_objects as graph_objects
from collections import Counter
from sklearn.neighbors import KNeighborsClassifier as kNN
import seaborn as sns
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.linear_model import LogisticRegression

from google.colab import drive
drive.mount('/content/drive')

dataset = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/DataSet/Project/healthcare-dataset-stroke-data.csv")

dataset

dataset.info()

dataset.describe()

dataset.isnull().sum()

# filling missing values in the dataset
avg1 = dataset[dataset['stroke'] == 1]['bmi'].mean()
df2 = dataset[dataset['stroke'] == 1].fillna(avg1)
avg0 = dataset[dataset['stroke'] == 0]['bmi'].mean()
df1 = dataset[dataset['stroke'] == 0].fillna(avg0)
dataset = pd.concat([df1,df2], axis = 0)

df = dataset.copy(deep = True)

"""## Exploratory Discriminant Analysis"""

counts0 = Counter(df[df['stroke']==0]['work_type'])
counts1 = Counter(df[df['stroke']==1]['work_type'])
fig = make_subplots(rows=1, cols=2, specs=[[{"type": "pie"}, {"type": "pie"}]])

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts0.items())],
     labels=[item[0] for item in sorted(counts0.items())],
     domain=dict(x=[0, 0.5]),
     name="Stroke Negative",title='Stroke Negative'), 
     row=1, col=1)

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts1.items())],
     labels=[item[0] for item in sorted(counts1.items())],
     domain=dict(x=[0.5, 1.0]),
     name="Stroke Positive",title='Stroke Positive'),
    row=1, col=2)

fig.update_layout(
    title={'text':'Stroke vs Worktype','xanchor':'left','yanchor': 'top','y':0.9,'x':0.35},
    xaxis_title="X Axis Title",
    yaxis_title="Y Axis Title",
    legend_title="Worktype",
    font=dict(size=18)
)

fig.show()

counts0 = Counter(df[df['stroke']==0]['gender'])
counts1 = Counter(df[df['stroke']==1]['gender'])
fig = make_subplots(rows=1, cols=2, specs=[[{"type": "pie"}, {"type": "pie"}]])

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts0.items())],
     labels=[item[0] for item in sorted(counts0.items())],
     domain=dict(x=[0, 0.5]),
     name="Stroke Negative",title='Stroke Negative'), 
     row=1, col=1)

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts1.items())],
     labels=[item[0] for item in sorted(counts1.items())],
     domain=dict(x=[0.5, 1.0]),
     name="Stroke Positive",title='Stroke Positive'),
    row=1, col=2)

fig.update_layout(
    title={'text':'Stroke vs Gender','xanchor':'left','yanchor': 'top','y':0.9,'x':0.35},
    xaxis_title="X Axis Title",
    yaxis_title="Y Axis Title",
    legend_title="Gender",
    font=dict(size=18)
)

fig.show()

counts0 = Counter(df[df['stroke']==0]['Residence_type'])
counts1 = Counter(df[df['stroke']==1]['Residence_type'])
fig = make_subplots(rows=1, cols=2, specs=[[{"type": "pie"}, {"type": "pie"}]])

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts0.items())],
     labels=[item[0] for item in sorted(counts0.items())],
     domain=dict(x=[0, 0.5]),
     name="Stroke Negative",title='Stroke Negative'), 
     row=1, col=1)

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts1.items())],
     labels=[item[0] for item in sorted(counts1.items())],
     domain=dict(x=[0.5, 1.0]),
     name="Stroke Positive",title='Stroke Positive'),
    row=1, col=2)

fig.update_layout(
    title={'text':'Stroke vs Residence type','xanchor':'left','yanchor': 'top','y':0.9,'x':0.35},
    xaxis_title="X Axis Title",
    yaxis_title="Y Axis Title",
    legend_title="Residence type",
    font=dict(size=18)
)

fig.show()

counts0 = Counter(df[df['stroke']==0]['hypertension'])
counts1 = Counter(df[df['stroke']==1]['hypertension'])
fig = make_subplots(rows=1, cols=2, specs=[[{"type": "pie"}, {"type": "pie"}]])

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts0.items())],
     labels=[item[0] for item in sorted(counts0.items())],
     domain=dict(x=[0, 0.5]),
     name="Stroke Negative",title='Stroke Negative'), 
     row=1, col=1)

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts1.items())],
     labels=[item[0] for item in sorted(counts1.items())],
     domain=dict(x=[0.5, 1.0]),
     name="Stroke Positive",title='Stroke Positive'),
    row=1, col=2)

fig.update_layout(
    title={'text':'Stroke vs hypertension','xanchor':'left','yanchor': 'top','y':0.9,'x':0.35},
    xaxis_title="X Axis Title",
    yaxis_title="Y Axis Title",
    legend_title="hypertension",
    font=dict(size=18)
)

fig.show()

counts0 = Counter(df[df['stroke']==0]['ever_married'])
counts1 = Counter(df[df['stroke']==1]['ever_married'])
fig = make_subplots(rows=1, cols=2, specs=[[{"type": "pie"}, {"type": "pie"}]])

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts0.items())],
     labels=[item[0] for item in sorted(counts0.items())],
     domain=dict(x=[0, 0.5]),
     name="Stroke Negative",title='Stroke Negative'), 
     row=1, col=1)

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts1.items())],
     labels=[item[0] for item in sorted(counts1.items())],
     domain=dict(x=[0.5, 1.0]),
     name="Stroke Positive",title='Stroke Positive'),
    row=1, col=2)

fig.update_layout(
    title={'text':'Stroke vs marriage status','xanchor':'left','yanchor': 'top','y':0.9,'x':0.35},
    xaxis_title="X Axis Title",
    yaxis_title="Y Axis Title",
    legend_title="marriage status",
    font=dict(size=18)
)

fig.show()

counts0 = Counter(df[df['stroke']==0]['heart_disease'])
counts1 = Counter(df[df['stroke']==1]['heart_disease'])
fig = make_subplots(rows=1, cols=2, specs=[[{"type": "pie"}, {"type": "pie"}]])

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts0.items())],
     labels=[item[0] for item in sorted(counts0.items())],
     domain=dict(x=[0, 0.5]),
     name="Stroke Negative",title='Stroke Negative'), 
     row=1, col=1)

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts1.items())],
     labels=[item[0] for item in sorted(counts1.items())],
     domain=dict(x=[0.5, 1.0]),
     name="Stroke Positive",title='Stroke Positive'),
    row=1, col=2)

fig.update_layout(
    title={'text':'Stroke vs Heart Disease','xanchor':'left','yanchor': 'top','y':0.9,'x':0.35},
    xaxis_title="X Axis Title",
    yaxis_title="Y Axis Title",
    legend_title="Heart disease",
    font=dict(size=18)
)

fig.show()

counts0 = Counter(df[df['stroke']==0]['smoking_status'])
counts1 = Counter(df[df['stroke']==1]['smoking_status'])
fig = make_subplots(rows=1, cols=2, specs=[[{"type": "pie"}, {"type": "pie"}]])

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts0.items())],
     labels=[item[0] for item in sorted(counts0.items())],
     domain=dict(x=[0, 0.5]),
     name="Stroke Negative",title='Stroke Negative'), 
     row=1, col=1)

fig.add_trace(graph_objects.Pie(
     values=[item[1] for item in sorted(counts1.items())],
     labels=[item[0] for item in sorted(counts1.items())],
     domain=dict(x=[0.5, 1.0]),
     name="Stroke Positive",title='Stroke Positive'),
    row=1, col=2)

fig.update_layout(
    title={'text':'Stroke vs Smoking Status','xanchor':'left','yanchor': 'top','y':0.9,'x':0.35},
    xaxis_title="X Axis Title",
    yaxis_title="Y Axis Title",
    legend_title="Smoking Status",
    font=dict(size=18)
)

fig.show()

glucose_bins = np.linspace(0,280,29)
bmi_bins = np.linspace(0,100,51)
age_bins = np.linspace(0,90,10)
df['binned_glucose'] = pd.cut(df['avg_glucose_level'], glucose_bins, labels=glucose_bins[:-1],right=False)
df['binned_bmi'] = pd.cut(df['bmi'], bmi_bins, labels=bmi_bins[:-1],right=False)
df['binned_age'] = pd.cut(df['age'], age_bins, labels=age_bins[:-1],right=False)
df['binned_glucose'] = df['binned_glucose'].astype('int')
df['binned_bmi'] = df['binned_bmi'].astype('int')
df['binned_age'] = df['binned_age'].astype('int')

sns.set(rc={'figure.figsize':(9,5)})
ax = sns.countplot(x='binned_age',hue='stroke',data=df)
ax.set_title('Stroke/Non stroke cases at various age groups').set_fontsize(22)
ax.set_xlabel('Age',fontsize=14)
ax.set_ylabel('Count',fontsize=14)

sns.set(rc={'figure.figsize':(9,5)})
ax = sns.countplot(x='binned_glucose',hue='stroke',data=df)
ax.set_title('Stroke/Non stroke cases at various glucose Levels').set_fontsize(22)
ax.set_xlabel('Glucose level',fontsize=14)
ax.set_ylabel('Count',fontsize=14)

sns.set(rc={'figure.figsize':(9,5)})
ax = sns.countplot(x='binned_bmi',hue='stroke',data = df)
ax.set_title('Stroke/Non stroke cases at various bmi Levels').set_fontsize(22)
ax.set_xlabel('bmi level',fontsize=14)
ax.set_ylabel('Count',fontsize=14)

stroke_cases = Counter(df['binned_glucose'][df['stroke']==1])
total_cases = Counter(df['binned_glucose'])
print(stroke_cases.items())
print(total_cases.items())
print(np.array([item[1] for item in sorted(stroke_cases.items())]).shape)
stroke_pct = 100*np.array([item[1] for item in sorted(stroke_cases.items())])/np.array([item[1] for item in sorted(total_cases.items())])
labels = [item[0] for item in sorted(total_cases.items())]
plt.style.use('seaborn')
plt.figure(figsize=(9,5))
plt.ylim(0,30)
plt.title('Stroke risk vs Glucose level (All age groups)',fontsize=20)
plt.ylabel('% of strokes',fontsize=15)
plt.xlabel('Average glucose level',fontsize=15)
plt.plot(labels[:-1],stroke_pct[:-1],'b')

stroke_cases = Counter(df['binned_age'][df['stroke']==1])
total_cases = Counter(df['binned_age'])
stroke_cases[20] = 0
stroke_pct = 100*np.array([item[1] for item in sorted(stroke_cases.items())])/np.array([item[1] for item in sorted(total_cases.items())])
labels = [item[0] for item in sorted(total_cases.items())]
plt.style.use('seaborn')
plt.figure(figsize=(9,5))
plt.ylim(0,30)
plt.title('Stroke risk vs age level (All age groups)',fontsize=20)
plt.ylabel('% of strokes',fontsize=15)
plt.xlabel('Average age level',fontsize=15)
plt.plot(labels[:-1],stroke_pct[:-1],'b')

stroke_cases = Counter(df['binned_bmi'][df['stroke']==1])
total_cases = Counter(df['binned_bmi'])
for key in total_cases:
    if(key not in stroke_cases.keys()):
        stroke_cases[key] = 0
stroke_pct = 100*np.array([item[1] for item in sorted(stroke_cases.items())])/np.array([item[1] for item in sorted(total_cases.items())])
labels = [item[0] for item in sorted(total_cases.items())]
plt.style.use('seaborn')
plt.figure(figsize=(9,5))
plt.ylim(0,30)
plt.title('Stroke risk vs BMI level (All age groups)',fontsize=20)
plt.ylabel('% of strokes',fontsize=15)
plt.xlabel('BMI level',fontsize=15)
plt.plot(labels[:-1],stroke_pct[:-1],'b')

dataset.isnull().sum()

dataset.drop(['id'], axis = 1, inplace = True)

dataset.head()

dataset.info()

# normalizing continous feature columns
scaler = StandardScaler()
for feature in dataset.columns:
    if(feature == 'age' or feature == 'avg_glucose_level' or feature == 'bmi'):
        dataset[[feature]] = scaler.fit_transform(dataset[[feature]])
    else:
        continue
dataset

# Label Encoding of Categorical features
for feature in dataset.columns:
    if(dataset[feature].dtype == 'object'):
        dataset[feature] = LabelEncoder().fit_transform(dataset[feature])

dataset

# Splitting the dataset into train and test
dataset_train, dataset_test = train_test_split(dataset, random_state = 42, test_size = 0.2, stratify = dataset.stroke)

# splitting the train and test data into inputs(features) and outputs(targets)
X_train = dataset_train.iloc[:,:10]
y_train = dataset_train.iloc[:,-1]
X_test = dataset_test.iloc[:,:10]
y_test = dataset_test.iloc[:,-1]

# Sampling of the dataset since, number of positive samples are very low in the original dataset
counter = Counter(y_train)
print('Before', counter)
oversampler = RandomOverSampler(sampling_strategy='minority')
x_sm, y_sm = oversampler.fit_resample(X_train, y_train)
counter = Counter(y_sm)
print("After", counter)

"""#Models"""

# function for specificity and sensitivity
def calc_sens_spec(y_true,y_pred):
    conf_matrix = confusion_matrix(y_true,y_pred)
    TP = conf_matrix[1][1]
    TN = conf_matrix[0][0]
    FP = conf_matrix[0][1]
    FN = conf_matrix[1][0]
    # calculate the sensitivity
    sensitivity = TP / (TP + FN)    
    # calculate the specificity
    specificity = TN / (TN + FP)
    return sensitivity,specificity

models = []
models_accuracy = []
models_sensitivity = []
models_specificity = []
models_f1_score = []
classifiers = []

# custom loss function to prioritize both sensitivity and specificity  
def custom_loss_function1(y_test,pred):
    sens = calc_sens_spec(y_test,pred)[0]
    spec = calc_sens_spec(y_test,pred)[1]
    return 2*sens*spec/(sens + spec)

def append_scores(model,y_test,y_pred):
    models.append(model)
    models_accuracy.append(accuracy_score(y_test, y_pred))
    models_sensitivity.append(calc_sens_spec(y_test,y_pred)[0])
    models_specificity.append(calc_sens_spec(y_test,y_pred)[1])
    models_f1_score.append(f1_score(y_test, y_pred))
    return

def show_analysis(model,y_test,pred,y_prob):
    print("Classification report for {}: \n{}".format(model,classification_report(y_test,pred)))
    print("Confusion matrix for {}: \n{}".format(model,confusion_matrix(y_test,pred)))
    print("Accuracy score for {}: {:.2f}".format(model,accuracy_score(y_test,pred)))
    # calculate sensitivity, specificity, and auc
    sens_dtree,spec_dtree = calc_sens_spec(y_test,pred)
    fpr, tpr, _ = roc_curve(y_test,  y_prob[:,1])
    auc_dtree = roc_auc_score(y_test, y_prob[:,1])
    print("Sensitivity score for {}: {:.2f}".format(model,sens_dtree))
    print("Specitivity score for {}: {:.2f}".format(model,spec_dtree))
    print("AUC score for {}: {:.2f}".format(model,auc_dtree))
    fig, ax = plt.subplots()
    ax.plot(fpr, tpr, color='blue', label='ROC curve (area = %0.2f)' % auc_dtree)
    ax.plot([0, 1], [0, 1], color='green', linestyle='--')
    ax.set_xlim([-0.05, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title('Receiver Operating Characteristic {}'.format(model))
    ax.legend(loc="lower right")
    plt.show()
    return

"""##DT"""

# Training Decision Tree Classifier
model = DecisionTreeClassifier()
model.fit(x_sm, y_sm)
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)
classifiers.append(model)

append_scores('DT',y_test,y_pred)
show_analysis('DT',y_test,y_pred,y_prob)

# # Grid Search for Decision tree Classifier
params = {
        'criterion': ['gini','entropy'],
        'min_samples_leaf': range(1,3),
        'max_depth': range(2,6)
        }
scorer = make_scorer(custom_loss_function1,greater_is_better=True)
clf = GridSearchCV(DecisionTreeClassifier(), params, scoring = scorer)
clf.fit(x_sm,y_sm)
# clf.cv_results_
print(clf.best_score_)
print(clf.best_params_)

clf = DecisionTreeClassifier(criterion = 'gini', max_depth = 5, min_samples_leaf = 1)
clf.fit(x_sm,y_sm)
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)
show_analysis('Grid search DT',y_test,y_pred,y_prob)
append_scores('Grid search DT', y_test,y_pred)

"""##XGB"""

# Training XGBoost Classifier
model = XGBClassifier()
model.fit(x_sm, y_sm)
y_pred =model.predict(X_test)
y_prob = model.predict_proba(X_test)
classifiers.append(model)
# Evaluation of the model
append_scores('XGB',y_test,y_pred)
show_analysis('XGB',y_test,y_pred,y_prob)

# grid search for XGBoost model
params = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1.5, 2.5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8],
        'max_depth': [3, 4, 5]
        }
scorer = make_scorer(custom_loss_function1,greater_is_better=True)
clf = GridSearchCV(XGBClassifier(), params, scoring = scorer)
clf.fit(x_sm,y_sm)
# print(clf.cv_results_)
print(clf.best_score_)
print(clf.best_params_)

clf = XGBClassifier(probability = True,colsample_bytree = 0.8, gamma = 0.5, max_depth = 5, min_child_weight = 1, subsample = 0.6)
clf.fit(x_sm,y_sm)
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)
show_analysis('Grid search XGB',y_test,y_pred,y_prob)
append_scores('Grid search XGB', y_test, y_pred)

"""##Random Forest"""

# Training Random Forest Classifer
model = RandomForestClassifier()
model.fit(x_sm, y_sm)
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)
classifiers.append(model)

# Evaluation of the model

append_scores('RFC',y_test,y_pred)
show_analysis('RFC',y_test,y_pred,y_prob)

params = {

'max_depth': [3,5,7],

'max_features': ['auto', 'sqrt'],

'min_samples_leaf': [1, 2, 4],

'min_samples_split': [2, 5, 10],

'n_estimators': [50,100,150]

}
scorer = make_scorer(custom_loss_function1,greater_is_better=True)
clf = GridSearchCV(RandomForestClassifier(), params, scoring = scorer)
clf.fit(x_sm,y_sm)
# print(clf.cv_results_)
print(clf.best_score_)
print(clf.best_params_)

clf = RandomForestClassifier(max_depth = 7, max_features = 'sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 50)
clf.fit(x_sm,y_sm)
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)

show_analysis('Grid search RFC',y_test,y_pred,y_prob)
append_scores('Grid search RFC', y_test, y_pred)

"""##LGBM"""

model = lgb.LGBMClassifier()
model.fit(x_sm, y_sm)
y_pred =model.predict(X_test)
y_prob = model.predict_proba(X_test)
classifiers.append(model)

append_scores('LGBM',y_test,y_pred)
show_analysis('LGBM',y_test,y_pred,y_prob)

params = {

        'bagging_fraction': [0.8,0.9,1.0],
        'max_depth': [ 5, 7, 9, 11],
        'num_leaves': [200, 300, 400],
        'n_estimators': [150,200,300]
}
scorer = make_scorer(custom_loss_function1,greater_is_better=True)
clf = GridSearchCV(lgb.LGBMClassifier(), params, scoring = scorer)
clf.fit(x_sm,y_sm)
# print(clf.cv_results_)
print(clf.best_score_)
print(clf.best_params_)

clf = lgb.LGBMClassifier(bagging_fraction= 0.8, max_depth= 11, n_estimators= 300, num_leaves= 200,probability = True)
clf.fit(x_sm,y_sm)
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)

show_analysis('Grid search LGBM',y_test,y_pred,y_prob)
append_scores('Grid search LGBM', y_test, y_pred)

"""##LinearSVC"""

model = SVC(kernel='linear', probability=True)
model.fit(x_sm, y_sm)
y_pred =model.predict(X_test)
y_prob = model.predict_proba(X_test)
classifiers.append(model)

append_scores('Linear SVC',y_test,y_pred)
print(confusion_matrix(y_test, y_pred))

show_analysis('Linear SVC',y_test,y_pred,y_prob)

"""##RBF"""

model = SVC(kernel='rbf', probability=True) 
model.fit(x_sm, y_sm)
y_pred =model.predict(X_test)
y_prob = model.predict_proba(X_test)
classifiers.append(model)

append_scores('RBF',y_test,y_pred)

show_analysis('SVC RBF',y_test,y_pred,y_prob)

params = {'C': [0.1, 1, 10],
              'gamma': [1, 0.1, 0.01],
              'kernel': ['rbf',]}
scorer = make_scorer(custom_loss_function1,greater_is_better=True)
clf = GridSearchCV(SVC(), params, scoring = scorer)
clf.fit(x_sm,y_sm)
# print(clf.cv_results_)
print(clf.best_score_)
print(clf.best_params_)

clf = SVC(probability = True,C = 10,gamma = 1,kernel = 'rbf')
clf.fit(x_sm,y_sm)
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)

show_analysis('Grid search SVC RBF',y_test,y_pred,y_prob)
append_scores('Grid search SVC RBF', y_test, y_pred)

"""##LDA"""

model = LinearDiscriminantAnalysis() 
model.fit(x_sm, y_sm)
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)
classifiers.append(model)

append_scores('LDA',y_test,y_pred)

show_analysis('LDA',y_test,y_pred,y_prob)

"""##QDA"""

model = QuadraticDiscriminantAnalysis() 
model.fit(x_sm, y_sm)
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)
classifiers.append(model)

append_scores('QDA', y_test,y_pred)

show_analysis('QDA',y_test,y_pred,y_prob)

"""## knn"""

model = kNN()
model.fit(x_sm, y_sm)
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)
classifiers.append(model)

append_scores('kNN',y_test,y_pred)

show_analysis('kNN',y_test,y_pred,y_prob)

"""## MLP"""

model = MLPClassifier(solver = 'sgd')
model.fit(x_sm, y_sm)
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)
classifiers.append(model)

append_scores('MLP',y_test,y_pred)

show_analysis('MLP',y_test,y_pred,y_prob)

"""# LR"""

model = LogisticRegression() 
model.fit(x_sm, y_sm)
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)
classifiers.append(model)

append_scores('LR',y_test,y_pred)

show_analysis('LR',y_test,y_pred,y_prob)

params = {
    'solver' : ['newton-cg', 'lbfgs', 'liblinear'],
    'penalty' : ['none', 'l1', 'l2', 'elasticnet'],
    'C' : [1e-5, 1e-3, 1e-1, 1]
}

scorer = make_scorer(custom_loss_function1,greater_is_better=True)
clf = GridSearchCV(LogisticRegression(), params, scoring = scorer)
clf.fit(x_sm,y_sm)
# print(clf.cv_results_)
print(clf.best_score_)
print(clf.best_params_)

clf = LogisticRegression(C = 0.001, penalty = 'l2', solver = 'newton-cg')
clf.fit(x_sm,y_sm)
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)

show_analysis('Grid search LR',y_test,y_pred,y_prob)
append_scores('Grid search LR', y_test, y_pred)

"""##Comparison"""

df = pd.DataFrame({'models': models, 'accuracy': models_accuracy, 'sensitivity': models_sensitivity, 'specificity': models_specificity})
df

def Bagging(input, models):
    cumulative_pred = np.zeros((len(input)), dtype = int)

    for i in range(len(models)):
        cumulative_pred += np.array(models[i].predict(input))

    cumulative_pred = cumulative_pred > len(models)//2
    cumulative_pred = cumulative_pred*1

    return cumulative_pred.tolist();

pred = Bagging(X_test, classifiers)

print("Classification report for Bagging: \n{}".format(classification_report(y_test,pred)))
print("Confusion matrix for Bagging: \n{}".format(confusion_matrix(y_test,pred)))
print("Accuracy score for Bagging: {:.2f}".format(accuracy_score(y_test,pred)))

"""#Machine Learning Model Deployment Code"""

! pip install streamlit

import pickle

model = classifiers[-1]

filename = 'trained_model.sav'
pickle.dump(model, open(filename, 'wb'))

# loading the saved model
loaded_model = pickle.load(open('trained_model.sav', 'rb'))

input_data = (1,0.874525,0,0,1,2,0,-0.616568,0.765941,2)

# changing the input_data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

prediction = loaded_model.predict_proba(input_data_reshaped)[:,1][0]
print(f"You have {prediction*100}% chance of having a stroke.")